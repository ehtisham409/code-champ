{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehtisham409/code-champ/blob/master/simple_sytem_of_equation_byab_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWr_kmsD4sp"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math, random, cmath\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import gamma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq8mYHVDq_ki"
      },
      "source": [
        "def sampler_index(l_arr1,h_arr1,num):\n",
        "  \"\"\"this sampler function returns pair of randomly sampled index elements from tuples of ranges provided in input\"\"\"\n",
        "\n",
        "  samples = np.zeros((num,1),dtype=int)\n",
        "  for i in range(num):\n",
        "    samples[i,0] = np.random.randint(l_arr1,h_arr1)\n",
        "\n",
        "  return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBz4Dav-BM8C"
      },
      "source": [
        "# Specifying number of input samples M for values of x and N for values of t\n",
        "M = 50\n",
        "# Limits for  -1 <= x <= 1\n",
        "x_l = 0.0\n",
        "x_h = 3.0\n",
        "h = (x_h - x_l) / M\n",
        "x = np.arange(x_l, x_h, h)\n",
        "# Initial conditions for g1 and g2 functions\n",
        "init_x_g1 = tf.Variable(np.array([[x_l]]))\n",
        "g1_init_x = 0.0\n",
        "\n",
        "init_x_g2 = tf.Variable(np.array([[x_l]]))\n",
        "g2_init_x = 1.0\n",
        "\n",
        "\n",
        "# Define the number of neurons in each layer for g1 function\n",
        "n_nodes_hl1_g1 = 50\n",
        "n_nodes_hl2_g1 = 50\n",
        "n_nodes_hl3_g1 = 20\n",
        "n_nodes_hl4_g1 = 20\n",
        "n_nodes_hl5_g1 = 20\n",
        "# Define the number of neurons in each layer for g2 function\n",
        "n_nodes_hl1_g2 = 50\n",
        "n_nodes_hl2_g2 = 50\n",
        "n_nodes_hl3_g2 = 20\n",
        "n_nodes_hl4_g2 = 20\n",
        "n_nodes_hl5_g2 = 20\n",
        "\n",
        "# Define the number of outputs for g1 and g2 function\n",
        "n_classes_g1 = 1\n",
        "n_classes_g2 = 1\n",
        "\n",
        "# Defining hyperparameters for training\n",
        "learn_rate_g1 = 0.01\n",
        "learn_rate_g2 = 0.01\n",
        "\n",
        "epochs = 10000\n",
        "batch_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0aYwtmWEQzk"
      },
      "source": [
        "# Creating model for function g1 for training\n",
        "g1 = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(n_nodes_hl1_g1,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             tf.keras.layers.Dense(n_nodes_hl2_g1,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl3_g1,activation='', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl4_g1,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl5_g1,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             tf.keras.layers.Dense(n_classes_g1, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1))\n",
        "                            ])\n",
        "\n",
        "optimizer_g1 = tf.keras.optimizers.Adam(learn_rate_g1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juMsKPryxD6G"
      },
      "source": [
        "# Creating model for function g2 for training\n",
        "g2 = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(n_nodes_hl1_g2,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             tf.keras.layers.Dense(n_nodes_hl2_g2,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl3_g2,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl4_g2,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             #tf.keras.layers.Dense(n_nodes_hl5_g2,activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1)),\n",
        "                             tf.keras.layers.Dense(n_classes_g2, kernel_initializer=tf.keras.initializers.GlorotNormal(seed=1))\n",
        "                            ])\n",
        "\n",
        "optimizer_g2 = tf.keras.optimizers.Adam(learn_rate_g2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-hzXnFGlX8a"
      },
      "source": [
        "def differential_eq_1(g1, g2, x, sample_indices, h):\n",
        "  diff_op = 0.0\n",
        "  for s in range(len(sample_indices)):\n",
        "    i = sample_indices[s][0]\n",
        "    x_i = x[i]\n",
        "    x_tensor = tf.Variable(np.array([[x_i]]),dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(x_tensor)\n",
        "      g1_x = g1(x_tensor)\n",
        "      g2_x = g2(x_tensor)\n",
        "    pde_g1 = tape.gradient(g1_x, x_tensor)\n",
        "\n",
        "    pde_g1_x = pde_g1[0][0]\n",
        "\n",
        "    diff_op += (pde_g1_x - tf.cos(x_tensor)[0][0] - ((g1_x[0][0]) ** 2) - g2_x[0][0] + (1 + ((x_tensor[0][0]) ** 2) + ((tf.sin(x_tensor)[0][0]) ** 2)))\n",
        "\n",
        "  return diff_op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52TGw8U4H_87"
      },
      "source": [
        "def differential_eq_2(g1, g2, x, sample_indices, h):\n",
        "  diff_op = 0.0\n",
        "  for s in range(len(sample_indices)):\n",
        "    i = sample_indices[s][0]\n",
        "    x_i = x[i]\n",
        "    x_tensor = tf.Variable(np.array([[x_i]]),dtype=tf.float32)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(x_tensor)\n",
        "      g1_x = g1(x_tensor)\n",
        "      g2_x = g2(x_tensor)\n",
        "      # pde_g1 is getting two derivatives w.r.t both x_i and t_n\n",
        "    pde_g2 = tape.gradient(g2_x, x_tensor)\n",
        "    # Partial derivative of \"G1\" w.r.t \"t\"\n",
        "    pde_g2_x = pde_g2[0][0]\n",
        "\n",
        "    diff_op += (pde_g2_x - (2 * x_tensor[0][0]) + ((1 + (x_tensor[0][0] ** 2)) * tf.sin(x_tensor)[0][0])  - (g1_x[0][0]) * (g2_x[0][0]))\n",
        "\n",
        "  return diff_op"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAZJJab5pfC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac32084-42de-47bb-a500-20236756825b"
      },
      "source": [
        "\n",
        "for iteration in range(epochs):\n",
        "    #Sampling for both differential equations\n",
        "    samples_de = sampler_index(1, M, batch_size)\n",
        "    # Sampling for initial condition of g1 and g2\n",
        "    #ic_g1 = 0*tf.ones_like(init_x_g1)\n",
        "    #ic_g2 = 0*tf.ones_like(init_x_g2)\n",
        "\n",
        "    with tf.GradientTape() as tape_g1:\n",
        "      loss_de_g1 = differential_eq_1(g1, g2, x, samples_de, h)\n",
        "      loss_de_g2 = differential_eq_2(g1, g2, x, samples_de, h)\n",
        "      loss_ic_g1 = batch_size * (g1(init_x_g1)[0][0] - g1_init_x)\n",
        "      #######################################################################\n",
        "      # Total loss of g1\n",
        "      loss_g1 = (loss_de_g1 ** 2)+ (loss_de_g2 ** 2) + (loss_ic_g1 ** 2)\n",
        "     ########################################################################\n",
        "      # Calculating gradients\n",
        "      grads_g1 = tape_g1.gradient(loss_g1, g1.trainable_variables)\n",
        "\n",
        "    with tf.GradientTape() as tape_g2:\n",
        "      loss_de_g1 = differential_eq_1(g1, g2, x, samples_de, h)\n",
        "      loss_de_g2 = differential_eq_2(g1, g2, x, samples_de, h)\n",
        "\n",
        "      loss_ic_g2 = batch_size * (g2(init_x_g2)[0][0] - g2_init_x)\n",
        "      ##################################################################\n",
        "      # Total loss of g2\n",
        "      loss_g2 = (loss_de_g2 ** 2) + (loss_de_g2 ** 2) + (loss_ic_g2 ** 2)\n",
        "      ####################################################################\n",
        "      # Calculating gradients\n",
        "      grads_g2 = tape_g2.gradient(loss_g2, g2.trainable_variables)\n",
        "\n",
        "    if loss_g1/batch_size < 0.001 and loss_g2/batch_size < 0.001:\n",
        "      break\n",
        "\n",
        "    # Applying gradients to the both g1 and g2 models\n",
        "    optimizer_g1.apply_gradients(zip(grads_g1, g1.trainable_variables))\n",
        "    optimizer_g2.apply_gradients(zip(grads_g2, g2.trainable_variables))\n",
        "\n",
        "    if (iteration + 1) % 10 == 0:\n",
        "      print(\"Epoch {:4d}: g1 has loss {:4.5f} and g2 has loss {:4.5f}\".format(iteration + 1, loss_g1/batch_size, loss_g2/batch_size))\n",
        "\n",
        "print(\"\\n##########################################################################################\\n\")\n",
        "print(\"After running for {:4d} epochs, g1 has final loss {:4.5f} and g2 has final loss {:4.5f}\".format(iteration + 1, loss_g1/batch_size, loss_g2/batch_size))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   10: g1 has loss 9.35072 and g2 has loss 12.13941\n",
            "Epoch   20: g1 has loss 26.23582 and g2 has loss 32.28160\n",
            "Epoch   30: g1 has loss 1.38746 and g2 has loss 3.18165\n",
            "Epoch   40: g1 has loss 1.69424 and g2 has loss 3.37546\n",
            "Epoch   50: g1 has loss 0.10769 and g2 has loss 1.25577\n",
            "Epoch   60: g1 has loss 6.44562 and g2 has loss 9.14196\n",
            "Epoch   70: g1 has loss 6.19706 and g2 has loss 8.64406\n",
            "Epoch   80: g1 has loss 1.05470 and g2 has loss 2.45442\n",
            "Epoch   90: g1 has loss 2.83279 and g2 has loss 1.00628\n",
            "Epoch  100: g1 has loss 4.93566 and g2 has loss 8.41721\n",
            "Epoch  110: g1 has loss 19.29555 and g2 has loss 32.30039\n",
            "Epoch  120: g1 has loss 0.45388 and g2 has loss 0.55564\n",
            "Epoch  130: g1 has loss 2.85339 and g2 has loss 3.98171\n",
            "Epoch  140: g1 has loss 0.57354 and g2 has loss 1.84142\n",
            "Epoch  150: g1 has loss 1.12649 and g2 has loss 0.73251\n",
            "Epoch  160: g1 has loss 0.16773 and g2 has loss 0.35383\n",
            "Epoch  170: g1 has loss 0.19075 and g2 has loss 1.16537\n",
            "Epoch  180: g1 has loss 0.65012 and g2 has loss 2.38730\n",
            "Epoch  190: g1 has loss 12.32809 and g2 has loss 11.86650\n",
            "Epoch  200: g1 has loss 3.03768 and g2 has loss 3.54565\n",
            "Epoch  210: g1 has loss 2.37795 and g2 has loss 4.64436\n",
            "Epoch  220: g1 has loss 0.90649 and g2 has loss 1.98048\n",
            "Epoch  230: g1 has loss 4.92498 and g2 has loss 7.56356\n",
            "Epoch  240: g1 has loss 1.42519 and g2 has loss 2.42946\n",
            "Epoch  250: g1 has loss 3.03725 and g2 has loss 5.42916\n",
            "Epoch  260: g1 has loss 5.78049 and g2 has loss 7.30248\n",
            "Epoch  270: g1 has loss 5.11441 and g2 has loss 11.14169\n",
            "Epoch  280: g1 has loss 2.29751 and g2 has loss 2.37052\n",
            "Epoch  290: g1 has loss 7.31425 and g2 has loss 0.94941\n",
            "Epoch  300: g1 has loss 4.19364 and g2 has loss 4.00616\n",
            "Epoch  310: g1 has loss 12.40268 and g2 has loss 22.25171\n",
            "Epoch  320: g1 has loss 1.52765 and g2 has loss 2.65211\n",
            "Epoch  330: g1 has loss 3.32294 and g2 has loss 0.78183\n",
            "Epoch  340: g1 has loss 9.24317 and g2 has loss 13.65636\n",
            "Epoch  350: g1 has loss 1.75893 and g2 has loss 3.53970\n",
            "Epoch  360: g1 has loss 0.16600 and g2 has loss 0.50295\n",
            "Epoch  370: g1 has loss 1.43197 and g2 has loss 0.91620\n",
            "Epoch  380: g1 has loss 1.54169 and g2 has loss 1.94272\n",
            "Epoch  390: g1 has loss 1.06682 and g2 has loss 2.42011\n",
            "Epoch  400: g1 has loss 0.75395 and g2 has loss 1.59379\n",
            "Epoch  410: g1 has loss 1.92415 and g2 has loss 1.30817\n",
            "Epoch  420: g1 has loss 2.04991 and g2 has loss 4.51803\n",
            "Epoch  430: g1 has loss 8.01902 and g2 has loss 11.60340\n",
            "Epoch  440: g1 has loss 1.88245 and g2 has loss 3.39621\n",
            "Epoch  450: g1 has loss 0.32832 and g2 has loss 0.34001\n",
            "Epoch  460: g1 has loss 1.58435 and g2 has loss 3.71306\n",
            "Epoch  470: g1 has loss 3.86770 and g2 has loss 1.93550\n",
            "Epoch  480: g1 has loss 0.52382 and g2 has loss 2.09062\n",
            "Epoch  490: g1 has loss 0.17498 and g2 has loss 0.97126\n",
            "Epoch  500: g1 has loss 25.02175 and g2 has loss 32.92427\n",
            "Epoch  510: g1 has loss 5.03638 and g2 has loss 1.53824\n",
            "Epoch  520: g1 has loss 3.49324 and g2 has loss 2.34392\n",
            "Epoch  530: g1 has loss 4.37858 and g2 has loss 5.07922\n",
            "Epoch  540: g1 has loss 0.83747 and g2 has loss 2.24943\n",
            "Epoch  550: g1 has loss 0.78398 and g2 has loss 1.93599\n",
            "Epoch  560: g1 has loss 1.60402 and g2 has loss 3.06420\n",
            "Epoch  570: g1 has loss 1.64050 and g2 has loss 3.12285\n",
            "Epoch  580: g1 has loss 0.35469 and g2 has loss 2.33565\n",
            "Epoch  590: g1 has loss 1.81578 and g2 has loss 1.95804\n",
            "Epoch  600: g1 has loss 8.03278 and g2 has loss 14.43110\n",
            "Epoch  610: g1 has loss 0.47915 and g2 has loss 0.45630\n",
            "Epoch  620: g1 has loss 1.06735 and g2 has loss 0.60730\n",
            "Epoch  630: g1 has loss 1.06425 and g2 has loss 1.47933\n",
            "Epoch  640: g1 has loss 0.45131 and g2 has loss 0.90226\n",
            "Epoch  650: g1 has loss 2.57230 and g2 has loss 5.53440\n",
            "Epoch  660: g1 has loss 0.66111 and g2 has loss 1.91296\n",
            "Epoch  670: g1 has loss 0.99628 and g2 has loss 3.67776\n",
            "Epoch  680: g1 has loss 1.55762 and g2 has loss 4.19320\n",
            "Epoch  690: g1 has loss 0.41740 and g2 has loss 1.06685\n",
            "Epoch  700: g1 has loss 1.24332 and g2 has loss 1.68429\n",
            "Epoch  710: g1 has loss 1.52817 and g2 has loss 3.44167\n",
            "Epoch  720: g1 has loss 0.49994 and g2 has loss 1.24758\n",
            "Epoch  730: g1 has loss 2.36439 and g2 has loss 4.24097\n",
            "Epoch  740: g1 has loss 0.79631 and g2 has loss 1.85748\n",
            "Epoch  750: g1 has loss 3.07514 and g2 has loss 4.29900\n",
            "Epoch  760: g1 has loss 1.81041 and g2 has loss 3.37141\n",
            "Epoch  770: g1 has loss 0.55725 and g2 has loss 1.17805\n",
            "Epoch  780: g1 has loss 0.43195 and g2 has loss 1.51072\n",
            "Epoch  790: g1 has loss 4.06661 and g2 has loss 4.57729\n",
            "Epoch  800: g1 has loss 1.97147 and g2 has loss 1.76406\n",
            "Epoch  810: g1 has loss 0.53570 and g2 has loss 1.34678\n",
            "Epoch  820: g1 has loss 5.20195 and g2 has loss 4.12531\n",
            "Epoch  830: g1 has loss 1.95528 and g2 has loss 1.51265\n",
            "Epoch  840: g1 has loss 1.32570 and g2 has loss 1.94487\n",
            "Epoch  850: g1 has loss 0.14474 and g2 has loss 0.61186\n",
            "Epoch  860: g1 has loss 3.39341 and g2 has loss 4.20316\n",
            "Epoch  870: g1 has loss 1.47410 and g2 has loss 3.00284\n",
            "Epoch  880: g1 has loss 0.63499 and g2 has loss 1.40155\n",
            "Epoch  890: g1 has loss 6.88643 and g2 has loss 10.56136\n",
            "Epoch  900: g1 has loss 3.49799 and g2 has loss 1.65560\n",
            "Epoch  910: g1 has loss 1.79793 and g2 has loss 2.16817\n",
            "Epoch  920: g1 has loss 7.86771 and g2 has loss 8.64940\n",
            "Epoch  930: g1 has loss 3.47111 and g2 has loss 0.86423\n",
            "Epoch  940: g1 has loss 2.27730 and g2 has loss 1.16979\n",
            "Epoch  950: g1 has loss 6.93125 and g2 has loss 6.37542\n",
            "Epoch  960: g1 has loss 0.73805 and g2 has loss 1.32889\n",
            "Epoch  970: g1 has loss 1.42656 and g2 has loss 3.10698\n",
            "Epoch  980: g1 has loss 0.42641 and g2 has loss 1.02890\n",
            "Epoch  990: g1 has loss 0.12285 and g2 has loss 0.19715\n",
            "Epoch 1000: g1 has loss 6.19494 and g2 has loss 10.80131\n",
            "Epoch 1010: g1 has loss 4.85316 and g2 has loss 6.41784\n",
            "Epoch 1020: g1 has loss 0.38261 and g2 has loss 1.22508\n",
            "Epoch 1030: g1 has loss 0.38050 and g2 has loss 1.03210\n",
            "Epoch 1040: g1 has loss 1.02900 and g2 has loss 2.82344\n",
            "Epoch 1050: g1 has loss 3.21109 and g2 has loss 2.08154\n",
            "Epoch 1060: g1 has loss 12.25089 and g2 has loss 4.98887\n",
            "Epoch 1070: g1 has loss 1.24472 and g2 has loss 2.13557\n",
            "Epoch 1080: g1 has loss 6.29363 and g2 has loss 6.66742\n",
            "Epoch 1090: g1 has loss 0.59905 and g2 has loss 3.01326\n",
            "Epoch 1100: g1 has loss 6.79066 and g2 has loss 3.24203\n",
            "Epoch 1110: g1 has loss 2.74415 and g2 has loss 3.36336\n",
            "Epoch 1120: g1 has loss 1.91006 and g2 has loss 3.82161\n",
            "Epoch 1130: g1 has loss 0.87249 and g2 has loss 0.92618\n",
            "Epoch 1140: g1 has loss 9.43977 and g2 has loss 10.82595\n",
            "Epoch 1150: g1 has loss 1.56561 and g2 has loss 3.96831\n",
            "Epoch 1160: g1 has loss 1.84557 and g2 has loss 4.51725\n",
            "Epoch 1170: g1 has loss 0.19846 and g2 has loss 1.97672\n",
            "Epoch 1180: g1 has loss 0.88690 and g2 has loss 0.66174\n",
            "Epoch 1190: g1 has loss 1.84527 and g2 has loss 3.54803\n",
            "Epoch 1200: g1 has loss 0.43427 and g2 has loss 1.84028\n",
            "Epoch 1210: g1 has loss 2.36947 and g2 has loss 3.43035\n",
            "Epoch 1220: g1 has loss 1.98579 and g2 has loss 4.89027\n",
            "Epoch 1230: g1 has loss 5.77435 and g2 has loss 11.67282\n",
            "Epoch 1240: g1 has loss 7.74727 and g2 has loss 15.05118\n",
            "Epoch 1250: g1 has loss 0.28309 and g2 has loss 0.43551\n",
            "Epoch 1260: g1 has loss 0.94421 and g2 has loss 1.51329\n",
            "Epoch 1270: g1 has loss 0.19937 and g2 has loss 1.99819\n",
            "Epoch 1280: g1 has loss 1.10500 and g2 has loss 0.70421\n",
            "Epoch 1290: g1 has loss 4.81660 and g2 has loss 4.47985\n",
            "Epoch 1300: g1 has loss 3.59698 and g2 has loss 5.58698\n",
            "Epoch 1310: g1 has loss 1.60712 and g2 has loss 3.97577\n",
            "Epoch 1320: g1 has loss 2.12249 and g2 has loss 4.04440\n",
            "Epoch 1330: g1 has loss 1.65704 and g2 has loss 2.14744\n",
            "Epoch 1340: g1 has loss 0.99724 and g2 has loss 1.74987\n",
            "Epoch 1350: g1 has loss 1.08795 and g2 has loss 1.74373\n",
            "Epoch 1360: g1 has loss 3.56962 and g2 has loss 5.85862\n",
            "Epoch 1370: g1 has loss 2.16669 and g2 has loss 3.35158\n",
            "Epoch 1380: g1 has loss 3.13161 and g2 has loss 4.25891\n",
            "Epoch 1390: g1 has loss 0.39877 and g2 has loss 1.52764\n",
            "Epoch 1400: g1 has loss 0.12853 and g2 has loss 0.92640\n",
            "Epoch 1410: g1 has loss 1.46196 and g2 has loss 1.97062\n",
            "Epoch 1420: g1 has loss 0.08370 and g2 has loss 0.58371\n",
            "Epoch 1430: g1 has loss 0.21696 and g2 has loss 0.55527\n",
            "Epoch 1440: g1 has loss 1.98063 and g2 has loss 4.28784\n",
            "Epoch 1450: g1 has loss 0.96985 and g2 has loss 3.58603\n",
            "Epoch 1460: g1 has loss 2.98756 and g2 has loss 4.49482\n",
            "Epoch 1470: g1 has loss 2.06873 and g2 has loss 4.14380\n",
            "Epoch 1480: g1 has loss 0.16796 and g2 has loss 1.53565\n",
            "Epoch 1490: g1 has loss 4.21319 and g2 has loss 8.41958\n",
            "Epoch 1500: g1 has loss 1.14283 and g2 has loss 1.19076\n",
            "Epoch 1510: g1 has loss 0.28156 and g2 has loss 1.83758\n",
            "Epoch 1520: g1 has loss 1.67274 and g2 has loss 2.12647\n",
            "Epoch 1530: g1 has loss 4.24429 and g2 has loss 6.14657\n",
            "Epoch 1540: g1 has loss 2.96423 and g2 has loss 5.48528\n",
            "Epoch 1550: g1 has loss 1.17584 and g2 has loss 3.41410\n",
            "Epoch 1560: g1 has loss 0.29284 and g2 has loss 2.45881\n",
            "Epoch 1570: g1 has loss 0.76689 and g2 has loss 1.33633\n",
            "Epoch 1580: g1 has loss 0.15638 and g2 has loss 0.87458\n",
            "Epoch 1590: g1 has loss 4.37752 and g2 has loss 7.50519\n",
            "Epoch 1600: g1 has loss 0.18752 and g2 has loss 1.06713\n",
            "Epoch 1610: g1 has loss 1.42222 and g2 has loss 1.45430\n",
            "Epoch 1620: g1 has loss 0.43340 and g2 has loss 0.97630\n",
            "Epoch 1630: g1 has loss 6.58623 and g2 has loss 8.44249\n",
            "Epoch 1640: g1 has loss 6.07319 and g2 has loss 12.78510\n",
            "Epoch 1650: g1 has loss 2.18527 and g2 has loss 1.30462\n",
            "Epoch 1660: g1 has loss 2.73932 and g2 has loss 6.15390\n",
            "Epoch 1670: g1 has loss 1.56293 and g2 has loss 3.43661\n",
            "Epoch 1680: g1 has loss 3.01815 and g2 has loss 0.79456\n",
            "Epoch 1690: g1 has loss 1.33276 and g2 has loss 1.38620\n",
            "Epoch 1700: g1 has loss 0.52806 and g2 has loss 0.50698\n",
            "Epoch 1710: g1 has loss 0.55307 and g2 has loss 1.18520\n",
            "Epoch 1720: g1 has loss 1.45612 and g2 has loss 3.25660\n",
            "Epoch 1730: g1 has loss 1.86555 and g2 has loss 1.98246\n",
            "Epoch 1740: g1 has loss 1.45025 and g2 has loss 3.64278\n",
            "Epoch 1750: g1 has loss 0.33970 and g2 has loss 1.69748\n",
            "Epoch 1760: g1 has loss 5.01152 and g2 has loss 6.74900\n",
            "Epoch 1770: g1 has loss 1.45007 and g2 has loss 3.95493\n",
            "Epoch 1780: g1 has loss 1.18723 and g2 has loss 2.36657\n",
            "Epoch 1790: g1 has loss 2.64437 and g2 has loss 3.95094\n",
            "Epoch 1800: g1 has loss 1.53815 and g2 has loss 3.04748\n",
            "Epoch 1810: g1 has loss 0.19098 and g2 has loss 0.85342\n",
            "Epoch 1820: g1 has loss 1.53825 and g2 has loss 1.99110\n",
            "Epoch 1830: g1 has loss 0.86804 and g2 has loss 1.45630\n",
            "Epoch 1840: g1 has loss 4.74437 and g2 has loss 6.25110\n",
            "Epoch 1850: g1 has loss 0.05963 and g2 has loss 0.80714\n",
            "Epoch 1860: g1 has loss 3.80767 and g2 has loss 6.65708\n",
            "Epoch 1870: g1 has loss 10.31195 and g2 has loss 15.95874\n",
            "Epoch 1880: g1 has loss 3.68485 and g2 has loss 3.73879\n",
            "Epoch 1890: g1 has loss 4.19808 and g2 has loss 1.00686\n",
            "Epoch 1900: g1 has loss 0.35295 and g2 has loss 0.53301\n",
            "Epoch 1910: g1 has loss 1.24855 and g2 has loss 3.40850\n",
            "Epoch 1920: g1 has loss 5.07631 and g2 has loss 7.15996\n",
            "Epoch 1930: g1 has loss 5.53392 and g2 has loss 10.49881\n",
            "Epoch 1940: g1 has loss 0.70037 and g2 has loss 0.93094\n",
            "Epoch 1950: g1 has loss 1.25625 and g2 has loss 2.16391\n",
            "Epoch 1960: g1 has loss 0.75853 and g2 has loss 1.16097\n",
            "Epoch 1970: g1 has loss 2.25302 and g2 has loss 4.38202\n",
            "Epoch 1980: g1 has loss 0.52100 and g2 has loss 0.95138\n",
            "Epoch 1990: g1 has loss 5.45595 and g2 has loss 8.30023\n",
            "Epoch 2000: g1 has loss 2.26182 and g2 has loss 4.29781\n",
            "Epoch 2010: g1 has loss 0.36364 and g2 has loss 1.52240\n",
            "Epoch 2020: g1 has loss 1.28684 and g2 has loss 2.29593\n",
            "Epoch 2030: g1 has loss 3.94919 and g2 has loss 8.30480\n",
            "Epoch 2040: g1 has loss 3.07073 and g2 has loss 2.00260\n",
            "Epoch 2050: g1 has loss 0.44182 and g2 has loss 1.13845\n",
            "Epoch 2060: g1 has loss 6.60537 and g2 has loss 9.22276\n",
            "Epoch 2070: g1 has loss 2.77892 and g2 has loss 4.77625\n",
            "Epoch 2080: g1 has loss 2.94438 and g2 has loss 3.07050\n",
            "Epoch 2090: g1 has loss 2.22739 and g2 has loss 2.26653\n",
            "Epoch 2100: g1 has loss 1.90037 and g2 has loss 4.31004\n",
            "Epoch 2110: g1 has loss 0.21606 and g2 has loss 0.62759\n",
            "Epoch 2120: g1 has loss 0.51010 and g2 has loss 1.21797\n",
            "Epoch 2130: g1 has loss 1.77354 and g2 has loss 3.97446\n",
            "Epoch 2140: g1 has loss 0.48064 and g2 has loss 0.94349\n",
            "Epoch 2150: g1 has loss 3.43480 and g2 has loss 2.68925\n",
            "Epoch 2160: g1 has loss 3.78514 and g2 has loss 5.44469\n",
            "Epoch 2170: g1 has loss 3.21809 and g2 has loss 6.89007\n",
            "Epoch 2180: g1 has loss 0.43980 and g2 has loss 2.83062\n",
            "Epoch 2190: g1 has loss 2.44166 and g2 has loss 4.85305\n",
            "Epoch 2200: g1 has loss 6.30754 and g2 has loss 12.54873\n",
            "Epoch 2210: g1 has loss 0.37493 and g2 has loss 1.77881\n",
            "Epoch 2220: g1 has loss 2.42842 and g2 has loss 3.16200\n",
            "Epoch 2230: g1 has loss 0.23777 and g2 has loss 0.75410\n",
            "Epoch 2240: g1 has loss 0.12989 and g2 has loss 1.13684\n",
            "Epoch 2250: g1 has loss 0.74900 and g2 has loss 2.17322\n",
            "Epoch 2260: g1 has loss 6.72505 and g2 has loss 6.83119\n",
            "Epoch 2270: g1 has loss 0.28348 and g2 has loss 0.59856\n",
            "Epoch 2280: g1 has loss 0.38476 and g2 has loss 0.37991\n",
            "Epoch 2290: g1 has loss 0.56008 and g2 has loss 0.58598\n",
            "Epoch 2300: g1 has loss 3.61211 and g2 has loss 8.16999\n",
            "Epoch 2310: g1 has loss 5.63626 and g2 has loss 9.22180\n",
            "Epoch 2320: g1 has loss 1.08756 and g2 has loss 0.89841\n",
            "Epoch 2330: g1 has loss 4.31149 and g2 has loss 0.84363\n",
            "Epoch 2340: g1 has loss 1.72657 and g2 has loss 3.36203\n",
            "Epoch 2350: g1 has loss 3.79442 and g2 has loss 4.07181\n",
            "Epoch 2360: g1 has loss 0.51866 and g2 has loss 0.99042\n",
            "Epoch 2370: g1 has loss 0.17733 and g2 has loss 0.82008\n",
            "Epoch 2380: g1 has loss 2.32130 and g2 has loss 3.67963\n",
            "Epoch 2390: g1 has loss 0.84066 and g2 has loss 0.69529\n",
            "Epoch 2400: g1 has loss 2.11746 and g2 has loss 3.61304\n",
            "Epoch 2410: g1 has loss 4.48152 and g2 has loss 4.86154\n",
            "Epoch 2420: g1 has loss 0.56290 and g2 has loss 1.34710\n",
            "Epoch 2430: g1 has loss 2.71487 and g2 has loss 3.97546\n",
            "Epoch 2440: g1 has loss 2.00599 and g2 has loss 3.20970\n",
            "Epoch 2450: g1 has loss 0.11988 and g2 has loss 0.54101\n",
            "Epoch 2460: g1 has loss 0.77225 and g2 has loss 1.94186\n",
            "Epoch 2470: g1 has loss 3.80095 and g2 has loss 6.84795\n",
            "Epoch 2480: g1 has loss 8.91608 and g2 has loss 11.96601\n",
            "Epoch 2490: g1 has loss 0.48230 and g2 has loss 1.09448\n",
            "Epoch 2500: g1 has loss 2.77021 and g2 has loss 3.61733\n",
            "Epoch 2510: g1 has loss 0.25155 and g2 has loss 0.59608\n",
            "Epoch 2520: g1 has loss 1.48633 and g2 has loss 4.15116\n",
            "Epoch 2530: g1 has loss 0.82446 and g2 has loss 0.78966\n",
            "Epoch 2540: g1 has loss 1.51897 and g2 has loss 3.56678\n",
            "Epoch 2550: g1 has loss 0.54625 and g2 has loss 0.86520\n",
            "Epoch 2560: g1 has loss 0.49900 and g2 has loss 0.01406\n",
            "Epoch 2570: g1 has loss 5.17749 and g2 has loss 8.76937\n",
            "Epoch 2580: g1 has loss 2.29253 and g2 has loss 4.12235\n",
            "Epoch 2590: g1 has loss 6.58088 and g2 has loss 12.83676\n",
            "Epoch 2600: g1 has loss 0.99968 and g2 has loss 2.61436\n",
            "Epoch 2610: g1 has loss 0.89578 and g2 has loss 0.98858\n",
            "Epoch 2620: g1 has loss 2.28869 and g2 has loss 3.42060\n",
            "Epoch 2630: g1 has loss 3.53698 and g2 has loss 5.57622\n",
            "Epoch 2640: g1 has loss 3.67046 and g2 has loss 7.27956\n",
            "Epoch 2650: g1 has loss 5.01983 and g2 has loss 9.45311\n",
            "Epoch 2660: g1 has loss 0.28012 and g2 has loss 0.10469\n",
            "Epoch 2670: g1 has loss 1.92317 and g2 has loss 2.90365\n",
            "Epoch 2680: g1 has loss 0.91219 and g2 has loss 2.16944\n",
            "Epoch 2690: g1 has loss 0.40925 and g2 has loss 0.62581\n",
            "Epoch 2700: g1 has loss 6.53832 and g2 has loss 9.19112\n",
            "Epoch 2710: g1 has loss 3.47177 and g2 has loss 7.54999\n",
            "Epoch 2720: g1 has loss 0.27824 and g2 has loss 0.67548\n",
            "Epoch 2730: g1 has loss 2.75661 and g2 has loss 5.47911\n",
            "Epoch 2740: g1 has loss 1.39538 and g2 has loss 2.94312\n",
            "Epoch 2750: g1 has loss 5.11979 and g2 has loss 7.22279\n",
            "Epoch 2760: g1 has loss 0.04829 and g2 has loss 0.27311\n",
            "Epoch 2770: g1 has loss 2.30545 and g2 has loss 5.29296\n",
            "Epoch 2780: g1 has loss 6.33353 and g2 has loss 5.68438\n",
            "Epoch 2790: g1 has loss 2.02879 and g2 has loss 4.02503\n",
            "Epoch 2800: g1 has loss 3.92947 and g2 has loss 7.85746\n",
            "Epoch 2810: g1 has loss 0.14217 and g2 has loss 0.90089\n",
            "Epoch 2820: g1 has loss 5.49160 and g2 has loss 9.22057\n",
            "Epoch 2830: g1 has loss 1.06551 and g2 has loss 3.09196\n",
            "Epoch 2840: g1 has loss 0.78230 and g2 has loss 2.77056\n",
            "Epoch 2850: g1 has loss 0.88313 and g2 has loss 1.99499\n",
            "Epoch 2860: g1 has loss 3.22972 and g2 has loss 4.18934\n",
            "Epoch 2870: g1 has loss 2.07594 and g2 has loss 1.55836\n",
            "Epoch 2880: g1 has loss 1.18792 and g2 has loss 1.73401\n",
            "Epoch 2890: g1 has loss 2.07351 and g2 has loss 2.60283\n",
            "Epoch 2900: g1 has loss 0.94017 and g2 has loss 1.74156\n",
            "Epoch 2910: g1 has loss 1.02278 and g2 has loss 1.71811\n",
            "Epoch 2920: g1 has loss 0.09394 and g2 has loss 1.22102\n",
            "Epoch 2930: g1 has loss 2.15912 and g2 has loss 4.77044\n",
            "Epoch 2940: g1 has loss 3.64247 and g2 has loss 5.61066\n",
            "Epoch 2950: g1 has loss 0.73635 and g2 has loss 1.67367\n",
            "Epoch 2960: g1 has loss 1.22708 and g2 has loss 2.01872\n",
            "Epoch 2970: g1 has loss 4.77437 and g2 has loss 8.43774\n",
            "Epoch 2980: g1 has loss 3.36139 and g2 has loss 5.13695\n",
            "Epoch 2990: g1 has loss 0.82849 and g2 has loss 1.67110\n",
            "Epoch 3000: g1 has loss 11.90809 and g2 has loss 4.47265\n",
            "Epoch 3010: g1 has loss 2.48585 and g2 has loss 1.42630\n",
            "Epoch 3020: g1 has loss 3.58675 and g2 has loss 2.87835\n",
            "Epoch 3030: g1 has loss 7.97083 and g2 has loss 6.68321\n",
            "Epoch 3040: g1 has loss 1.96069 and g2 has loss 2.54480\n",
            "Epoch 3050: g1 has loss 2.65888 and g2 has loss 5.09721\n",
            "Epoch 3060: g1 has loss 1.79443 and g2 has loss 1.50251\n",
            "Epoch 3070: g1 has loss 2.90848 and g2 has loss 0.78553\n",
            "Epoch 3080: g1 has loss 3.51160 and g2 has loss 0.74122\n",
            "Epoch 3090: g1 has loss 1.71415 and g2 has loss 4.10035\n",
            "Epoch 3100: g1 has loss 1.93756 and g2 has loss 1.04235\n",
            "Epoch 3110: g1 has loss 2.35966 and g2 has loss 1.86416\n",
            "Epoch 3120: g1 has loss 4.33768 and g2 has loss 3.86022\n",
            "Epoch 3130: g1 has loss 0.89078 and g2 has loss 1.42201\n",
            "Epoch 3140: g1 has loss 0.14530 and g2 has loss 1.34574\n",
            "Epoch 3150: g1 has loss 0.66290 and g2 has loss 1.44910\n",
            "Epoch 3160: g1 has loss 0.38629 and g2 has loss 0.93508\n",
            "Epoch 3170: g1 has loss 0.24846 and g2 has loss 0.66239\n",
            "Epoch 3180: g1 has loss 0.36595 and g2 has loss 0.92523\n",
            "Epoch 3190: g1 has loss 0.05996 and g2 has loss 0.61132\n",
            "Epoch 3200: g1 has loss 2.67302 and g2 has loss 2.44081\n",
            "Epoch 3210: g1 has loss 0.48638 and g2 has loss 1.15146\n",
            "Epoch 3220: g1 has loss 0.23022 and g2 has loss 1.68895\n",
            "Epoch 3230: g1 has loss 2.33943 and g2 has loss 4.71093\n",
            "Epoch 3240: g1 has loss 0.23665 and g2 has loss 1.57289\n",
            "Epoch 3250: g1 has loss 4.91732 and g2 has loss 6.32221\n",
            "Epoch 3260: g1 has loss 1.47593 and g2 has loss 1.74048\n",
            "Epoch 3270: g1 has loss 3.37374 and g2 has loss 1.51061\n",
            "Epoch 3280: g1 has loss 4.18853 and g2 has loss 1.15941\n",
            "Epoch 3290: g1 has loss 5.50195 and g2 has loss 0.31556\n",
            "Epoch 3300: g1 has loss 0.38870 and g2 has loss 1.09037\n",
            "Epoch 3310: g1 has loss 0.20786 and g2 has loss 0.31589\n",
            "Epoch 3320: g1 has loss 0.76040 and g2 has loss 0.27950\n",
            "Epoch 3330: g1 has loss 0.34527 and g2 has loss 2.56171\n",
            "Epoch 3340: g1 has loss 0.22654 and g2 has loss 1.31190\n",
            "Epoch 3350: g1 has loss 0.84399 and g2 has loss 0.92116\n",
            "Epoch 3360: g1 has loss 1.91398 and g2 has loss 2.47254\n",
            "Epoch 3370: g1 has loss 1.92514 and g2 has loss 3.49156\n",
            "Epoch 3380: g1 has loss 4.07355 and g2 has loss 4.52097\n",
            "Epoch 3390: g1 has loss 4.59201 and g2 has loss 2.28894\n",
            "Epoch 3400: g1 has loss 4.56581 and g2 has loss 2.71348\n",
            "Epoch 3410: g1 has loss 5.60190 and g2 has loss 5.30915\n",
            "Epoch 3420: g1 has loss 7.91557 and g2 has loss 14.99219\n",
            "Epoch 3430: g1 has loss 9.72141 and g2 has loss 16.84906\n",
            "Epoch 3440: g1 has loss 5.46289 and g2 has loss 6.76886\n",
            "Epoch 3450: g1 has loss 1.95950 and g2 has loss 1.73412\n",
            "Epoch 3460: g1 has loss 0.68107 and g2 has loss 1.29475\n",
            "Epoch 3470: g1 has loss 2.03845 and g2 has loss 3.48622\n",
            "Epoch 3480: g1 has loss 2.88315 and g2 has loss 3.16187\n",
            "Epoch 3490: g1 has loss 9.87124 and g2 has loss 0.08777\n",
            "Epoch 3500: g1 has loss 0.83381 and g2 has loss 1.35922\n",
            "Epoch 3510: g1 has loss 1.96223 and g2 has loss 1.68100\n",
            "Epoch 3520: g1 has loss 0.56904 and g2 has loss 2.22728\n",
            "Epoch 3530: g1 has loss 1.16212 and g2 has loss 3.33853\n",
            "Epoch 3540: g1 has loss 1.59074 and g2 has loss 0.64161\n",
            "Epoch 3550: g1 has loss 0.92673 and g2 has loss 2.71073\n",
            "Epoch 3560: g1 has loss 2.80551 and g2 has loss 2.48537\n",
            "Epoch 3570: g1 has loss 1.42428 and g2 has loss 3.11519\n",
            "Epoch 3580: g1 has loss 0.62502 and g2 has loss 0.89619\n",
            "Epoch 3590: g1 has loss 4.08067 and g2 has loss 3.23159\n",
            "Epoch 3600: g1 has loss 1.17781 and g2 has loss 3.55822\n",
            "Epoch 3610: g1 has loss 9.57492 and g2 has loss 12.71650\n",
            "Epoch 3620: g1 has loss 0.90265 and g2 has loss 1.85804\n",
            "Epoch 3630: g1 has loss 0.85283 and g2 has loss 1.35614\n",
            "Epoch 3640: g1 has loss 5.14112 and g2 has loss 6.30469\n",
            "Epoch 3650: g1 has loss 1.13422 and g2 has loss 3.41729\n",
            "Epoch 3660: g1 has loss 1.33179 and g2 has loss 3.87185\n",
            "Epoch 3670: g1 has loss 2.14461 and g2 has loss 1.84462\n",
            "Epoch 3680: g1 has loss 4.19681 and g2 has loss 7.11624\n",
            "Epoch 3690: g1 has loss 1.55666 and g2 has loss 1.58299\n",
            "Epoch 3700: g1 has loss 9.49932 and g2 has loss 14.95144\n",
            "Epoch 3710: g1 has loss 2.05738 and g2 has loss 4.24028\n",
            "Epoch 3720: g1 has loss 7.80263 and g2 has loss 11.07004\n",
            "Epoch 3730: g1 has loss 4.06625 and g2 has loss 6.36553\n",
            "Epoch 3740: g1 has loss 0.40453 and g2 has loss 1.83489\n",
            "Epoch 3750: g1 has loss 0.05343 and g2 has loss 0.56654\n",
            "Epoch 3760: g1 has loss 6.95662 and g2 has loss 11.92548\n",
            "Epoch 3770: g1 has loss 9.57880 and g2 has loss 8.29176\n",
            "Epoch 3780: g1 has loss 1.34453 and g2 has loss 2.61403\n",
            "Epoch 3790: g1 has loss 0.32350 and g2 has loss 1.25929\n",
            "Epoch 3800: g1 has loss 0.76426 and g2 has loss 0.42775\n",
            "Epoch 3810: g1 has loss 4.29597 and g2 has loss 7.27546\n",
            "Epoch 3820: g1 has loss 1.54577 and g2 has loss 1.92409\n",
            "Epoch 3830: g1 has loss 1.52223 and g2 has loss 1.56596\n",
            "Epoch 3840: g1 has loss 1.57626 and g2 has loss 1.29106\n",
            "Epoch 3850: g1 has loss 1.28922 and g2 has loss 3.08654\n",
            "Epoch 3860: g1 has loss 1.88476 and g2 has loss 4.02733\n",
            "Epoch 3870: g1 has loss 1.06181 and g2 has loss 3.40479\n",
            "Epoch 3880: g1 has loss 2.21996 and g2 has loss 1.18758\n",
            "Epoch 3890: g1 has loss 2.90803 and g2 has loss 4.78801\n",
            "Epoch 3900: g1 has loss 0.53913 and g2 has loss 1.20335\n",
            "Epoch 3910: g1 has loss 1.17249 and g2 has loss 2.11263\n",
            "Epoch 3920: g1 has loss 0.55483 and g2 has loss 0.63778\n",
            "Epoch 3930: g1 has loss 2.12836 and g2 has loss 0.45261\n",
            "Epoch 3940: g1 has loss 3.32602 and g2 has loss 5.77942\n",
            "Epoch 3950: g1 has loss 1.04983 and g2 has loss 2.76486\n",
            "Epoch 3960: g1 has loss 0.21933 and g2 has loss 1.75902\n",
            "Epoch 3970: g1 has loss 0.40496 and g2 has loss 2.05892\n",
            "Epoch 3980: g1 has loss 0.62460 and g2 has loss 1.64017\n",
            "Epoch 3990: g1 has loss 0.78349 and g2 has loss 0.73515\n",
            "Epoch 4000: g1 has loss 3.19784 and g2 has loss 2.87753\n",
            "Epoch 4010: g1 has loss 1.71095 and g2 has loss 4.03654\n",
            "Epoch 4020: g1 has loss 0.36634 and g2 has loss 0.73173\n",
            "Epoch 4030: g1 has loss 8.98605 and g2 has loss 13.11643\n",
            "Epoch 4040: g1 has loss 0.72953 and g2 has loss 1.01913\n",
            "Epoch 4050: g1 has loss 0.80736 and g2 has loss 2.12774\n",
            "Epoch 4060: g1 has loss 1.56561 and g2 has loss 3.14437\n",
            "Epoch 4070: g1 has loss 0.49838 and g2 has loss 1.50688\n",
            "Epoch 4080: g1 has loss 0.78067 and g2 has loss 1.61810\n",
            "Epoch 4090: g1 has loss 0.55452 and g2 has loss 2.54439\n",
            "Epoch 4100: g1 has loss 0.69466 and g2 has loss 2.12061\n",
            "Epoch 4110: g1 has loss 0.60785 and g2 has loss 1.27310\n",
            "Epoch 4120: g1 has loss 0.20201 and g2 has loss 0.61869\n",
            "Epoch 4130: g1 has loss 1.31829 and g2 has loss 1.75215\n",
            "Epoch 4140: g1 has loss 3.81306 and g2 has loss 4.55181\n",
            "Epoch 4150: g1 has loss 0.72701 and g2 has loss 1.28152\n",
            "Epoch 4160: g1 has loss 1.31470 and g2 has loss 2.91563\n",
            "Epoch 4170: g1 has loss 0.59224 and g2 has loss 1.54848\n",
            "Epoch 4180: g1 has loss 8.75413 and g2 has loss 10.24743\n",
            "Epoch 4190: g1 has loss 2.78425 and g2 has loss 3.63389\n",
            "Epoch 4200: g1 has loss 0.54223 and g2 has loss 1.61781\n",
            "Epoch 4210: g1 has loss 2.31078 and g2 has loss 3.92769\n",
            "Epoch 4220: g1 has loss 1.94880 and g2 has loss 3.13715\n",
            "Epoch 4230: g1 has loss 0.40089 and g2 has loss 2.00331\n",
            "Epoch 4240: g1 has loss 2.16080 and g2 has loss 2.71826\n",
            "Epoch 4250: g1 has loss 1.07850 and g2 has loss 1.25000\n",
            "Epoch 4260: g1 has loss 0.40107 and g2 has loss 0.68239\n",
            "Epoch 4270: g1 has loss 0.97521 and g2 has loss 0.90491\n",
            "Epoch 4280: g1 has loss 3.83630 and g2 has loss 4.00741\n",
            "Epoch 4290: g1 has loss 5.82803 and g2 has loss 6.88814\n",
            "Epoch 4300: g1 has loss 0.93857 and g2 has loss 1.72994\n",
            "Epoch 4310: g1 has loss 0.39691 and g2 has loss 0.73075\n",
            "Epoch 4320: g1 has loss 7.17568 and g2 has loss 9.10998\n",
            "Epoch 4330: g1 has loss 0.70279 and g2 has loss 1.33705\n",
            "Epoch 4340: g1 has loss 0.43989 and g2 has loss 1.12624\n",
            "Epoch 4350: g1 has loss 0.90151 and g2 has loss 2.24475\n",
            "Epoch 4360: g1 has loss 4.38090 and g2 has loss 3.12309\n",
            "Epoch 4370: g1 has loss 5.91278 and g2 has loss 11.80810\n",
            "Epoch 4380: g1 has loss 4.71183 and g2 has loss 6.65309\n",
            "Epoch 4390: g1 has loss 2.39295 and g2 has loss 2.31653\n",
            "Epoch 4400: g1 has loss 0.60914 and g2 has loss 1.56948\n",
            "Epoch 4410: g1 has loss 0.24050 and g2 has loss 0.70836\n",
            "Epoch 4420: g1 has loss 0.44044 and g2 has loss 1.21157\n",
            "Epoch 4430: g1 has loss 2.86930 and g2 has loss 4.37037\n",
            "Epoch 4440: g1 has loss 1.16307 and g2 has loss 1.99378\n",
            "Epoch 4450: g1 has loss 6.69247 and g2 has loss 4.58055\n",
            "Epoch 4460: g1 has loss 3.19643 and g2 has loss 4.35098\n",
            "Epoch 4470: g1 has loss 1.56984 and g2 has loss 0.98080\n",
            "Epoch 4480: g1 has loss 0.78697 and g2 has loss 0.79577\n",
            "Epoch 4490: g1 has loss 0.87427 and g2 has loss 1.20529\n",
            "Epoch 4500: g1 has loss 7.92389 and g2 has loss 8.45114\n",
            "Epoch 4510: g1 has loss 1.76760 and g2 has loss 2.42160\n",
            "Epoch 4520: g1 has loss 2.39327 and g2 has loss 4.66257\n",
            "Epoch 4530: g1 has loss 1.21975 and g2 has loss 3.35673\n",
            "Epoch 4540: g1 has loss 0.74628 and g2 has loss 2.20589\n",
            "Epoch 4550: g1 has loss 6.11350 and g2 has loss 8.18580\n",
            "Epoch 4560: g1 has loss 0.71656 and g2 has loss 1.93216\n",
            "Epoch 4570: g1 has loss 0.54799 and g2 has loss 1.95022\n",
            "Epoch 4580: g1 has loss 3.49249 and g2 has loss 5.60602\n",
            "Epoch 4590: g1 has loss 1.33773 and g2 has loss 2.46177\n",
            "Epoch 4600: g1 has loss 0.67210 and g2 has loss 2.16033\n",
            "Epoch 4610: g1 has loss 5.88178 and g2 has loss 11.07425\n",
            "Epoch 4620: g1 has loss 3.45093 and g2 has loss 5.68226\n",
            "Epoch 4630: g1 has loss 2.03542 and g2 has loss 4.52269\n",
            "Epoch 4640: g1 has loss 1.82685 and g2 has loss 4.36830\n",
            "Epoch 4650: g1 has loss 8.46492 and g2 has loss 4.97714\n",
            "Epoch 4660: g1 has loss 0.51018 and g2 has loss 1.81009\n",
            "Epoch 4670: g1 has loss 3.73235 and g2 has loss 7.08690\n",
            "Epoch 4680: g1 has loss 0.75748 and g2 has loss 3.04771\n",
            "Epoch 4690: g1 has loss 0.55392 and g2 has loss 1.47064\n",
            "Epoch 4700: g1 has loss 7.90388 and g2 has loss 6.84547\n",
            "Epoch 4710: g1 has loss 5.00058 and g2 has loss 7.95910\n",
            "Epoch 4720: g1 has loss 1.20983 and g2 has loss 2.09736\n",
            "Epoch 4730: g1 has loss 2.98400 and g2 has loss 5.76303\n",
            "Epoch 4740: g1 has loss 0.18581 and g2 has loss 1.13467\n",
            "Epoch 4750: g1 has loss 0.11750 and g2 has loss 0.49390\n",
            "Epoch 4760: g1 has loss 0.59532 and g2 has loss 1.57975\n",
            "Epoch 4770: g1 has loss 4.09557 and g2 has loss 4.85107\n",
            "Epoch 4780: g1 has loss 3.08556 and g2 has loss 2.18471\n",
            "Epoch 4790: g1 has loss 0.19742 and g2 has loss 0.78619\n",
            "Epoch 4800: g1 has loss 1.98556 and g2 has loss 4.38655\n",
            "Epoch 4810: g1 has loss 3.15248 and g2 has loss 4.43024\n",
            "Epoch 4820: g1 has loss 1.68021 and g2 has loss 3.28133\n",
            "Epoch 4830: g1 has loss 6.97066 and g2 has loss 7.92999\n",
            "Epoch 4840: g1 has loss 3.07939 and g2 has loss 3.50263\n",
            "Epoch 4850: g1 has loss 0.16301 and g2 has loss 0.56503\n",
            "Epoch 4860: g1 has loss 7.50409 and g2 has loss 12.16763\n",
            "Epoch 4870: g1 has loss 0.85223 and g2 has loss 2.28279\n",
            "Epoch 4880: g1 has loss 2.43014 and g2 has loss 1.44929\n",
            "Epoch 4890: g1 has loss 0.45632 and g2 has loss 0.91860\n",
            "Epoch 4900: g1 has loss 6.92643 and g2 has loss 9.74590\n",
            "Epoch 4910: g1 has loss 3.23515 and g2 has loss 2.30479\n",
            "Epoch 4920: g1 has loss 0.43510 and g2 has loss 1.23881\n",
            "Epoch 4930: g1 has loss 0.52087 and g2 has loss 1.78024\n",
            "Epoch 4940: g1 has loss 1.49431 and g2 has loss 3.06667\n",
            "Epoch 4950: g1 has loss 0.47980 and g2 has loss 1.73953\n",
            "Epoch 4960: g1 has loss 10.91085 and g2 has loss 21.01978\n",
            "Epoch 4970: g1 has loss 1.33711 and g2 has loss 1.09590\n",
            "Epoch 4980: g1 has loss 4.59850 and g2 has loss 6.64930\n",
            "Epoch 4990: g1 has loss 2.47580 and g2 has loss 3.00495\n",
            "Epoch 5000: g1 has loss 4.67714 and g2 has loss 7.42462\n",
            "Epoch 5010: g1 has loss 1.61206 and g2 has loss 3.34708\n",
            "Epoch 5020: g1 has loss 0.61144 and g2 has loss 0.44903\n",
            "Epoch 5030: g1 has loss 1.45151 and g2 has loss 3.75790\n",
            "Epoch 5040: g1 has loss 0.43903 and g2 has loss 1.67447\n",
            "Epoch 5050: g1 has loss 2.45434 and g2 has loss 3.36864\n",
            "Epoch 5060: g1 has loss 13.09036 and g2 has loss 18.38900\n",
            "Epoch 5070: g1 has loss 1.21332 and g2 has loss 3.10831\n",
            "Epoch 5080: g1 has loss 1.92049 and g2 has loss 4.04114\n",
            "Epoch 5090: g1 has loss 0.78948 and g2 has loss 1.61044\n",
            "Epoch 5100: g1 has loss 2.19921 and g2 has loss 4.43236\n",
            "Epoch 5110: g1 has loss 1.41342 and g2 has loss 0.89721\n",
            "Epoch 5120: g1 has loss 8.79186 and g2 has loss 11.19318\n",
            "Epoch 5130: g1 has loss 2.12716 and g2 has loss 4.04771\n",
            "Epoch 5140: g1 has loss 10.10948 and g2 has loss 14.90270\n",
            "Epoch 5150: g1 has loss 0.62009 and g2 has loss 1.77868\n",
            "Epoch 5160: g1 has loss 1.46053 and g2 has loss 1.14431\n",
            "Epoch 5170: g1 has loss 2.07805 and g2 has loss 2.17325\n",
            "Epoch 5180: g1 has loss 0.18494 and g2 has loss 0.31280\n",
            "Epoch 5190: g1 has loss 0.90193 and g2 has loss 2.29677\n",
            "Epoch 5200: g1 has loss 5.75547 and g2 has loss 8.57407\n",
            "Epoch 5210: g1 has loss 1.10379 and g2 has loss 2.31247\n",
            "Epoch 5220: g1 has loss 2.52053 and g2 has loss 2.35228\n",
            "Epoch 5230: g1 has loss 2.36641 and g2 has loss 2.56127\n",
            "Epoch 5240: g1 has loss 1.37904 and g2 has loss 2.57733\n",
            "Epoch 5250: g1 has loss 1.64398 and g2 has loss 1.01955\n",
            "Epoch 5260: g1 has loss 2.82580 and g2 has loss 2.01875\n",
            "Epoch 5270: g1 has loss 1.68354 and g2 has loss 3.09691\n",
            "Epoch 5280: g1 has loss 2.91763 and g2 has loss 4.27902\n",
            "Epoch 5290: g1 has loss 6.26870 and g2 has loss 10.31367\n",
            "Epoch 5300: g1 has loss 0.56896 and g2 has loss 1.36069\n",
            "Epoch 5310: g1 has loss 3.53792 and g2 has loss 5.12366\n",
            "Epoch 5320: g1 has loss 1.15864 and g2 has loss 2.31580\n",
            "Epoch 5330: g1 has loss 0.14768 and g2 has loss 2.40201\n",
            "Epoch 5340: g1 has loss 8.38306 and g2 has loss 0.73891\n",
            "Epoch 5350: g1 has loss 6.56740 and g2 has loss 13.62493\n",
            "Epoch 5360: g1 has loss 7.50403 and g2 has loss 5.40743\n",
            "Epoch 5370: g1 has loss 1.27628 and g2 has loss 1.69889\n",
            "Epoch 5380: g1 has loss 1.57457 and g2 has loss 0.96754\n",
            "Epoch 5390: g1 has loss 3.06014 and g2 has loss 0.06511\n",
            "Epoch 5400: g1 has loss 2.38795 and g2 has loss 4.02175\n",
            "Epoch 5410: g1 has loss 3.09478 and g2 has loss 4.05577\n",
            "Epoch 5420: g1 has loss 1.88670 and g2 has loss 4.48425\n",
            "Epoch 5430: g1 has loss 2.00033 and g2 has loss 1.38656\n",
            "Epoch 5440: g1 has loss 2.29608 and g2 has loss 4.20585\n",
            "Epoch 5450: g1 has loss 0.82286 and g2 has loss 3.15713\n",
            "Epoch 5460: g1 has loss 2.00894 and g2 has loss 1.46522\n",
            "Epoch 5470: g1 has loss 1.08555 and g2 has loss 3.11699\n",
            "Epoch 5480: g1 has loss 1.33612 and g2 has loss 0.26193\n",
            "Epoch 5490: g1 has loss 0.07524 and g2 has loss 0.97995\n",
            "Epoch 5500: g1 has loss 0.74254 and g2 has loss 1.56358\n",
            "Epoch 5510: g1 has loss 1.93668 and g2 has loss 3.99145\n",
            "Epoch 5520: g1 has loss 2.26304 and g2 has loss 4.71873\n",
            "Epoch 5530: g1 has loss 1.11020 and g2 has loss 1.17183\n",
            "Epoch 5540: g1 has loss 3.28632 and g2 has loss 3.71502\n",
            "Epoch 5550: g1 has loss 0.49344 and g2 has loss 0.59703\n",
            "Epoch 5560: g1 has loss 11.97789 and g2 has loss 19.20977\n",
            "Epoch 5570: g1 has loss 1.05545 and g2 has loss 1.93450\n",
            "Epoch 5580: g1 has loss 1.20221 and g2 has loss 2.54640\n",
            "Epoch 5590: g1 has loss 0.56638 and g2 has loss 1.29533\n",
            "Epoch 5600: g1 has loss 4.92081 and g2 has loss 1.76902\n",
            "Epoch 5610: g1 has loss 6.93274 and g2 has loss 9.67585\n",
            "Epoch 5620: g1 has loss 2.93945 and g2 has loss 3.98382\n",
            "Epoch 5630: g1 has loss 0.33456 and g2 has loss 0.60767\n",
            "Epoch 5640: g1 has loss 7.40149 and g2 has loss 4.59242\n",
            "Epoch 5650: g1 has loss 30.01725 and g2 has loss 44.03369\n",
            "Epoch 5660: g1 has loss 3.31065 and g2 has loss 6.08085\n",
            "Epoch 5670: g1 has loss 10.96764 and g2 has loss 0.74225\n",
            "Epoch 5680: g1 has loss 6.03892 and g2 has loss 12.78722\n",
            "Epoch 5690: g1 has loss 6.24825 and g2 has loss 3.89883\n",
            "Epoch 5700: g1 has loss 4.34334 and g2 has loss 5.25255\n",
            "Epoch 5710: g1 has loss 3.85914 and g2 has loss 3.17743\n",
            "Epoch 5720: g1 has loss 9.73110 and g2 has loss 23.93876\n",
            "Epoch 5730: g1 has loss 3.41043 and g2 has loss 0.93458\n",
            "Epoch 5740: g1 has loss 3.83157 and g2 has loss 2.58424\n",
            "Epoch 5750: g1 has loss 6.64826 and g2 has loss 11.93266\n",
            "Epoch 5760: g1 has loss 1.60443 and g2 has loss 0.93860\n",
            "Epoch 5770: g1 has loss 6.02023 and g2 has loss 12.43232\n",
            "Epoch 5780: g1 has loss 0.21255 and g2 has loss 1.01020\n",
            "Epoch 5790: g1 has loss 0.44426 and g2 has loss 0.95894\n",
            "Epoch 5800: g1 has loss 1.89664 and g2 has loss 2.96791\n",
            "Epoch 5810: g1 has loss 2.47151 and g2 has loss 4.26431\n",
            "Epoch 5820: g1 has loss 2.87817 and g2 has loss 5.30252\n",
            "Epoch 5830: g1 has loss 2.85035 and g2 has loss 6.11509\n",
            "Epoch 5840: g1 has loss 0.39223 and g2 has loss 3.07064\n",
            "Epoch 5850: g1 has loss 0.46929 and g2 has loss 1.13788\n",
            "Epoch 5860: g1 has loss 5.60181 and g2 has loss 8.93513\n",
            "Epoch 5870: g1 has loss 1.55122 and g2 has loss 3.81458\n",
            "Epoch 5880: g1 has loss 0.55129 and g2 has loss 1.66837\n",
            "Epoch 5890: g1 has loss 1.81766 and g2 has loss 2.36906\n",
            "Epoch 5900: g1 has loss 0.05088 and g2 has loss 0.68582\n",
            "Epoch 5910: g1 has loss 1.25051 and g2 has loss 1.18303\n",
            "Epoch 5920: g1 has loss 2.87262 and g2 has loss 5.05652\n",
            "Epoch 5930: g1 has loss 2.21070 and g2 has loss 2.06919\n",
            "Epoch 5940: g1 has loss 0.06945 and g2 has loss 0.99128\n",
            "Epoch 5950: g1 has loss 0.29886 and g2 has loss 1.23075\n",
            "Epoch 5960: g1 has loss 1.13309 and g2 has loss 0.68662\n",
            "Epoch 5970: g1 has loss 8.68172 and g2 has loss 16.61386\n",
            "Epoch 5980: g1 has loss 0.25454 and g2 has loss 1.05308\n",
            "Epoch 5990: g1 has loss 3.07294 and g2 has loss 3.93494\n",
            "Epoch 6000: g1 has loss 4.78063 and g2 has loss 6.32505\n",
            "Epoch 6010: g1 has loss 0.98726 and g2 has loss 1.17057\n",
            "Epoch 6020: g1 has loss 4.41984 and g2 has loss 8.62013\n",
            "Epoch 6030: g1 has loss 1.35649 and g2 has loss 0.31075\n",
            "Epoch 6040: g1 has loss 4.11135 and g2 has loss 7.08633\n",
            "Epoch 6050: g1 has loss 0.22999 and g2 has loss 1.17533\n",
            "Epoch 6060: g1 has loss 3.09506 and g2 has loss 4.44295\n",
            "Epoch 6070: g1 has loss 1.10315 and g2 has loss 1.54101\n",
            "Epoch 6080: g1 has loss 3.36063 and g2 has loss 3.15191\n",
            "Epoch 6090: g1 has loss 0.98326 and g2 has loss 0.46908\n",
            "Epoch 6100: g1 has loss 8.09742 and g2 has loss 12.68646\n",
            "Epoch 6110: g1 has loss 3.47496 and g2 has loss 6.34390\n",
            "Epoch 6120: g1 has loss 1.10352 and g2 has loss 1.06491\n",
            "Epoch 6130: g1 has loss 0.42270 and g2 has loss 1.02483\n",
            "Epoch 6140: g1 has loss 8.48619 and g2 has loss 11.38470\n",
            "Epoch 6150: g1 has loss 0.31549 and g2 has loss 0.94285\n",
            "Epoch 6160: g1 has loss 1.21500 and g2 has loss 2.89753\n",
            "Epoch 6170: g1 has loss 1.16940 and g2 has loss 2.47165\n",
            "Epoch 6180: g1 has loss 3.82129 and g2 has loss 6.03550\n",
            "Epoch 6190: g1 has loss 1.71351 and g2 has loss 1.44460\n",
            "Epoch 6200: g1 has loss 1.91607 and g2 has loss 0.85768\n",
            "Epoch 6210: g1 has loss 1.34017 and g2 has loss 2.96083\n",
            "Epoch 6220: g1 has loss 9.19428 and g2 has loss 15.52878\n",
            "Epoch 6230: g1 has loss 1.00251 and g2 has loss 1.39761\n",
            "Epoch 6240: g1 has loss 8.61799 and g2 has loss 13.11943\n",
            "Epoch 6250: g1 has loss 4.69243 and g2 has loss 8.35481\n",
            "Epoch 6260: g1 has loss 1.76784 and g2 has loss 4.29683\n",
            "Epoch 6270: g1 has loss 0.88344 and g2 has loss 1.02959\n",
            "Epoch 6280: g1 has loss 11.58981 and g2 has loss 15.12991\n",
            "Epoch 6290: g1 has loss 0.52334 and g2 has loss 1.67694\n",
            "Epoch 6300: g1 has loss 4.48158 and g2 has loss 5.69741\n",
            "Epoch 6310: g1 has loss 6.80219 and g2 has loss 10.78473\n",
            "Epoch 6320: g1 has loss 3.99205 and g2 has loss 6.23864\n",
            "Epoch 6330: g1 has loss 1.54246 and g2 has loss 1.93442\n",
            "Epoch 6340: g1 has loss 3.44582 and g2 has loss 7.74572\n",
            "Epoch 6350: g1 has loss 3.32786 and g2 has loss 5.65162\n",
            "Epoch 6360: g1 has loss 1.00786 and g2 has loss 1.32219\n",
            "Epoch 6370: g1 has loss 0.14532 and g2 has loss 1.24015\n",
            "Epoch 6380: g1 has loss 1.16486 and g2 has loss 2.95597\n",
            "Epoch 6390: g1 has loss 0.91943 and g2 has loss 1.62525\n",
            "Epoch 6400: g1 has loss 2.06755 and g2 has loss 4.91813\n",
            "Epoch 6410: g1 has loss 1.84294 and g2 has loss 3.81075\n",
            "Epoch 6420: g1 has loss 10.43816 and g2 has loss 14.57148\n",
            "Epoch 6430: g1 has loss 2.61754 and g2 has loss 4.93182\n",
            "Epoch 6440: g1 has loss 0.42319 and g2 has loss 2.10364\n",
            "Epoch 6450: g1 has loss 1.86308 and g2 has loss 4.12144\n",
            "Epoch 6460: g1 has loss 0.46407 and g2 has loss 1.24736\n",
            "Epoch 6470: g1 has loss 0.31893 and g2 has loss 0.76649\n",
            "Epoch 6480: g1 has loss 4.50076 and g2 has loss 7.30246\n",
            "Epoch 6490: g1 has loss 6.11127 and g2 has loss 9.13678\n",
            "Epoch 6500: g1 has loss 1.36696 and g2 has loss 2.06705\n",
            "Epoch 6510: g1 has loss 1.56632 and g2 has loss 3.01700\n",
            "Epoch 6520: g1 has loss 2.55414 and g2 has loss 2.98163\n",
            "Epoch 6530: g1 has loss 3.46023 and g2 has loss 1.23153\n",
            "Epoch 6540: g1 has loss 0.48811 and g2 has loss 1.01654\n",
            "Epoch 6550: g1 has loss 0.04748 and g2 has loss 0.73985\n",
            "Epoch 6560: g1 has loss 0.32635 and g2 has loss 0.86767\n",
            "Epoch 6570: g1 has loss 2.14697 and g2 has loss 4.11966\n",
            "Epoch 6580: g1 has loss 0.64841 and g2 has loss 1.57125\n",
            "Epoch 6590: g1 has loss 1.63689 and g2 has loss 1.50170\n",
            "Epoch 6600: g1 has loss 0.56500 and g2 has loss 0.57544\n",
            "Epoch 6610: g1 has loss 8.54244 and g2 has loss 11.81908\n",
            "Epoch 6620: g1 has loss 0.29370 and g2 has loss 1.13393\n",
            "Epoch 6630: g1 has loss 3.80891 and g2 has loss 4.51651\n",
            "Epoch 6640: g1 has loss 1.89716 and g2 has loss 4.18811\n",
            "Epoch 6650: g1 has loss 2.74606 and g2 has loss 2.04590\n",
            "Epoch 6660: g1 has loss 0.56752 and g2 has loss 1.26026\n",
            "Epoch 6670: g1 has loss 0.10485 and g2 has loss 0.71768\n",
            "Epoch 6680: g1 has loss 0.80896 and g2 has loss 2.30836\n",
            "Epoch 6690: g1 has loss 1.25893 and g2 has loss 1.93337\n",
            "Epoch 6700: g1 has loss 2.74620 and g2 has loss 6.66098\n",
            "Epoch 6710: g1 has loss 0.48933 and g2 has loss 2.71459\n",
            "Epoch 6720: g1 has loss 0.35612 and g2 has loss 0.64417\n",
            "Epoch 6730: g1 has loss 1.37533 and g2 has loss 2.23307\n",
            "Epoch 6740: g1 has loss 9.88945 and g2 has loss 10.01541\n",
            "Epoch 6750: g1 has loss 1.83228 and g2 has loss 2.73261\n",
            "Epoch 6760: g1 has loss 3.30061 and g2 has loss 4.68161\n",
            "Epoch 6770: g1 has loss 8.17323 and g2 has loss 12.44718\n",
            "Epoch 6780: g1 has loss 0.40880 and g2 has loss 1.28619\n",
            "Epoch 6790: g1 has loss 9.07712 and g2 has loss 8.30877\n",
            "Epoch 6800: g1 has loss 0.25076 and g2 has loss 0.93990\n",
            "Epoch 6810: g1 has loss 1.03678 and g2 has loss 1.99797\n",
            "Epoch 6820: g1 has loss 0.68737 and g2 has loss 1.51384\n",
            "Epoch 6830: g1 has loss 0.77952 and g2 has loss 1.14702\n",
            "Epoch 6840: g1 has loss 5.44786 and g2 has loss 10.23408\n",
            "Epoch 6850: g1 has loss 0.66821 and g2 has loss 1.83042\n",
            "Epoch 6860: g1 has loss 1.87561 and g2 has loss 1.51291\n",
            "Epoch 6870: g1 has loss 1.15716 and g2 has loss 3.00318\n",
            "Epoch 6880: g1 has loss 1.94597 and g2 has loss 4.19601\n",
            "Epoch 6890: g1 has loss 0.86692 and g2 has loss 2.09047\n",
            "Epoch 6900: g1 has loss 1.98082 and g2 has loss 2.49166\n",
            "Epoch 6910: g1 has loss 5.52864 and g2 has loss 9.45404\n",
            "Epoch 6920: g1 has loss 0.79744 and g2 has loss 2.35800\n",
            "Epoch 6930: g1 has loss 2.02646 and g2 has loss 3.00383\n",
            "Epoch 6940: g1 has loss 8.48693 and g2 has loss 14.84626\n",
            "Epoch 6950: g1 has loss 1.08305 and g2 has loss 1.48122\n",
            "Epoch 6960: g1 has loss 3.37096 and g2 has loss 5.51676\n",
            "Epoch 6970: g1 has loss 0.78688 and g2 has loss 1.18821\n",
            "Epoch 6980: g1 has loss 0.74899 and g2 has loss 0.75711\n",
            "Epoch 6990: g1 has loss 0.75799 and g2 has loss 2.41735\n",
            "Epoch 7000: g1 has loss 2.82664 and g2 has loss 6.59331\n",
            "Epoch 7010: g1 has loss 0.97496 and g2 has loss 1.34261\n",
            "Epoch 7020: g1 has loss 4.95956 and g2 has loss 6.65486\n",
            "Epoch 7030: g1 has loss 2.46914 and g2 has loss 1.86965\n",
            "Epoch 7040: g1 has loss 1.27393 and g2 has loss 1.11296\n",
            "Epoch 7050: g1 has loss 1.38582 and g2 has loss 1.20606\n",
            "Epoch 7060: g1 has loss 0.49932 and g2 has loss 1.18763\n",
            "Epoch 7070: g1 has loss 4.16899 and g2 has loss 5.98150\n",
            "Epoch 7080: g1 has loss 2.70874 and g2 has loss 5.60093\n",
            "Epoch 7090: g1 has loss 1.75568 and g2 has loss 3.35067\n",
            "Epoch 7100: g1 has loss 1.61609 and g2 has loss 2.91689\n",
            "Epoch 7110: g1 has loss 1.93464 and g2 has loss 2.61558\n",
            "Epoch 7120: g1 has loss 0.90656 and g2 has loss 1.86498\n",
            "Epoch 7130: g1 has loss 0.81157 and g2 has loss 1.51889\n",
            "Epoch 7140: g1 has loss 0.17919 and g2 has loss 0.49331\n",
            "Epoch 7150: g1 has loss 0.82498 and g2 has loss 0.74672\n",
            "Epoch 7160: g1 has loss 0.22342 and g2 has loss 1.20417\n",
            "Epoch 7170: g1 has loss 3.04424 and g2 has loss 4.08996\n",
            "Epoch 7180: g1 has loss 2.06050 and g2 has loss 3.72865\n",
            "Epoch 7190: g1 has loss 1.24722 and g2 has loss 1.61030\n",
            "Epoch 7200: g1 has loss 1.76160 and g2 has loss 0.69708\n",
            "Epoch 7210: g1 has loss 0.20416 and g2 has loss 0.74632\n",
            "Epoch 7220: g1 has loss 4.72143 and g2 has loss 6.86523\n",
            "Epoch 7230: g1 has loss 0.62038 and g2 has loss 1.47945\n",
            "Epoch 7240: g1 has loss 2.81619 and g2 has loss 4.84611\n",
            "Epoch 7250: g1 has loss 0.39632 and g2 has loss 1.35426\n",
            "Epoch 7260: g1 has loss 3.51867 and g2 has loss 5.47842\n",
            "Epoch 7270: g1 has loss 4.82783 and g2 has loss 5.80859\n",
            "Epoch 7280: g1 has loss 3.02910 and g2 has loss 5.93289\n",
            "Epoch 7290: g1 has loss 0.54513 and g2 has loss 0.92253\n",
            "Epoch 7300: g1 has loss 3.56065 and g2 has loss 3.69814\n",
            "Epoch 7310: g1 has loss 1.27923 and g2 has loss 3.13214\n",
            "Epoch 7320: g1 has loss 2.45880 and g2 has loss 0.82065\n",
            "Epoch 7330: g1 has loss 4.94504 and g2 has loss 3.44441\n",
            "Epoch 7340: g1 has loss 0.24293 and g2 has loss 1.08120\n",
            "Epoch 7350: g1 has loss 2.28813 and g2 has loss 0.92651\n",
            "Epoch 7360: g1 has loss 0.07396 and g2 has loss 0.41866\n",
            "Epoch 7370: g1 has loss 3.75298 and g2 has loss 4.38189\n",
            "Epoch 7380: g1 has loss 7.89469 and g2 has loss 14.66209\n",
            "Epoch 7390: g1 has loss 8.27746 and g2 has loss 15.74285\n",
            "Epoch 7400: g1 has loss 0.66470 and g2 has loss 2.16776\n",
            "Epoch 7410: g1 has loss 5.13618 and g2 has loss 9.82142\n",
            "Epoch 7420: g1 has loss 3.79851 and g2 has loss 4.47871\n",
            "Epoch 7430: g1 has loss 22.52688 and g2 has loss 6.02623\n",
            "Epoch 7440: g1 has loss 1.79359 and g2 has loss 3.40693\n",
            "Epoch 7450: g1 has loss 4.31709 and g2 has loss 1.25018\n",
            "Epoch 7460: g1 has loss 3.10140 and g2 has loss 0.83894\n",
            "Epoch 7470: g1 has loss 1.13983 and g2 has loss 0.26646\n",
            "Epoch 7480: g1 has loss 0.30336 and g2 has loss 0.66734\n",
            "Epoch 7490: g1 has loss 4.30089 and g2 has loss 8.63732\n",
            "Epoch 7500: g1 has loss 1.34215 and g2 has loss 1.27843\n",
            "Epoch 7510: g1 has loss 2.82026 and g2 has loss 2.22044\n",
            "Epoch 7520: g1 has loss 2.54947 and g2 has loss 4.99474\n",
            "Epoch 7530: g1 has loss 8.61356 and g2 has loss 13.69972\n",
            "Epoch 7540: g1 has loss 5.47516 and g2 has loss 3.82054\n",
            "Epoch 7550: g1 has loss 0.61947 and g2 has loss 2.49078\n",
            "Epoch 7560: g1 has loss 3.20577 and g2 has loss 5.73589\n",
            "Epoch 7570: g1 has loss 8.85829 and g2 has loss 5.26503\n",
            "Epoch 7580: g1 has loss 0.32652 and g2 has loss 0.85603\n",
            "Epoch 7590: g1 has loss 3.07716 and g2 has loss 6.77016\n",
            "Epoch 7600: g1 has loss 6.03063 and g2 has loss 5.39311\n",
            "Epoch 7610: g1 has loss 1.93491 and g2 has loss 3.41544\n",
            "Epoch 7620: g1 has loss 1.44673 and g2 has loss 2.36125\n",
            "Epoch 7630: g1 has loss 0.98120 and g2 has loss 2.44394\n",
            "Epoch 7640: g1 has loss 0.40504 and g2 has loss 1.00011\n",
            "Epoch 7650: g1 has loss 0.15146 and g2 has loss 0.99660\n",
            "Epoch 7660: g1 has loss 2.88143 and g2 has loss 5.41362\n",
            "Epoch 7670: g1 has loss 3.09442 and g2 has loss 2.33123\n",
            "Epoch 7680: g1 has loss 7.02345 and g2 has loss 11.59508\n",
            "Epoch 7690: g1 has loss 3.13093 and g2 has loss 4.74981\n",
            "Epoch 7700: g1 has loss 3.34805 and g2 has loss 6.53685\n",
            "Epoch 7710: g1 has loss 10.14020 and g2 has loss 13.92553\n",
            "Epoch 7720: g1 has loss 0.88427 and g2 has loss 0.60601\n",
            "Epoch 7730: g1 has loss 0.09962 and g2 has loss 0.44114\n",
            "Epoch 7740: g1 has loss 0.27055 and g2 has loss 0.52365\n",
            "Epoch 7750: g1 has loss 0.83686 and g2 has loss 1.74903\n",
            "Epoch 7760: g1 has loss 0.83787 and g2 has loss 1.39246\n",
            "Epoch 7770: g1 has loss 1.68013 and g2 has loss 3.37663\n",
            "Epoch 7780: g1 has loss 0.35869 and g2 has loss 1.61316\n",
            "Epoch 7790: g1 has loss 2.10175 and g2 has loss 3.81147\n",
            "Epoch 7800: g1 has loss 1.88795 and g2 has loss 3.42365\n",
            "Epoch 7810: g1 has loss 8.26662 and g2 has loss 13.43340\n",
            "Epoch 7820: g1 has loss 7.00562 and g2 has loss 1.38853\n",
            "Epoch 7830: g1 has loss 0.13459 and g2 has loss 0.09360\n",
            "Epoch 7840: g1 has loss 1.28110 and g2 has loss 0.65567\n",
            "Epoch 7850: g1 has loss 1.09226 and g2 has loss 2.06510\n",
            "Epoch 7860: g1 has loss 1.41412 and g2 has loss 2.88552\n",
            "Epoch 7870: g1 has loss 0.93569 and g2 has loss 2.59474\n",
            "Epoch 7880: g1 has loss 1.55823 and g2 has loss 2.24405\n",
            "Epoch 7890: g1 has loss 2.81592 and g2 has loss 4.82032\n",
            "Epoch 7900: g1 has loss 0.07571 and g2 has loss 1.64496\n",
            "Epoch 7910: g1 has loss 2.52403 and g2 has loss 5.13993\n",
            "Epoch 7920: g1 has loss 1.02658 and g2 has loss 1.18125\n",
            "Epoch 7930: g1 has loss 1.28613 and g2 has loss 3.59861\n",
            "Epoch 7940: g1 has loss 2.94843 and g2 has loss 4.96035\n",
            "Epoch 7950: g1 has loss 0.54781 and g2 has loss 0.58813\n",
            "Epoch 7960: g1 has loss 3.15370 and g2 has loss 4.70661\n",
            "Epoch 7970: g1 has loss 0.57355 and g2 has loss 1.61605\n",
            "Epoch 7980: g1 has loss 4.11211 and g2 has loss 6.22002\n",
            "Epoch 7990: g1 has loss 3.60944 and g2 has loss 4.09312\n",
            "Epoch 8000: g1 has loss 3.13137 and g2 has loss 6.28206\n",
            "Epoch 8010: g1 has loss 1.79069 and g2 has loss 5.01000\n",
            "Epoch 8020: g1 has loss 1.22459 and g2 has loss 0.79804\n",
            "Epoch 8030: g1 has loss 1.38732 and g2 has loss 3.52888\n",
            "Epoch 8040: g1 has loss 2.28943 and g2 has loss 1.10137\n",
            "Epoch 8050: g1 has loss 2.01242 and g2 has loss 3.84465\n",
            "Epoch 8060: g1 has loss 1.29752 and g2 has loss 1.96951\n",
            "Epoch 8070: g1 has loss 0.94334 and g2 has loss 2.22575\n",
            "Epoch 8080: g1 has loss 0.92184 and g2 has loss 1.56330\n",
            "Epoch 8090: g1 has loss 5.55499 and g2 has loss 5.82214\n",
            "Epoch 8100: g1 has loss 3.48768 and g2 has loss 5.22663\n",
            "Epoch 8110: g1 has loss 1.57897 and g2 has loss 2.73540\n",
            "Epoch 8120: g1 has loss 1.31467 and g2 has loss 2.43898\n",
            "Epoch 8130: g1 has loss 0.33149 and g2 has loss 1.21510\n",
            "Epoch 8140: g1 has loss 3.06012 and g2 has loss 4.64089\n",
            "Epoch 8150: g1 has loss 0.50995 and g2 has loss 1.26561\n",
            "Epoch 8160: g1 has loss 2.12769 and g2 has loss 4.27577\n",
            "Epoch 8170: g1 has loss 2.87363 and g2 has loss 3.34643\n",
            "Epoch 8180: g1 has loss 1.07758 and g2 has loss 1.65641\n",
            "Epoch 8190: g1 has loss 6.40889 and g2 has loss 7.21454\n",
            "Epoch 8200: g1 has loss 2.91057 and g2 has loss 6.64533\n",
            "Epoch 8210: g1 has loss 3.39688 and g2 has loss 4.13156\n",
            "Epoch 8220: g1 has loss 1.45578 and g2 has loss 2.60677\n",
            "Epoch 8230: g1 has loss 0.46049 and g2 has loss 0.28130\n",
            "Epoch 8240: g1 has loss 3.02771 and g2 has loss 2.73192\n",
            "Epoch 8250: g1 has loss 1.59932 and g2 has loss 3.46032\n",
            "Epoch 8260: g1 has loss 2.61488 and g2 has loss 5.43135\n",
            "Epoch 8270: g1 has loss 0.45741 and g2 has loss 0.64043\n",
            "Epoch 8280: g1 has loss 0.39420 and g2 has loss 0.59933\n",
            "Epoch 8290: g1 has loss 1.53922 and g2 has loss 1.79619\n",
            "Epoch 8300: g1 has loss 0.84468 and g2 has loss 1.74781\n",
            "Epoch 8310: g1 has loss 0.77527 and g2 has loss 0.96561\n",
            "Epoch 8320: g1 has loss 1.69778 and g2 has loss 3.83615\n",
            "Epoch 8330: g1 has loss 6.57713 and g2 has loss 8.61727\n",
            "Epoch 8340: g1 has loss 0.65345 and g2 has loss 1.19787\n",
            "Epoch 8350: g1 has loss 0.70665 and g2 has loss 1.55166\n",
            "Epoch 8360: g1 has loss 0.05916 and g2 has loss 0.69789\n",
            "Epoch 8370: g1 has loss 0.94889 and g2 has loss 1.57792\n",
            "Epoch 8380: g1 has loss 1.41510 and g2 has loss 1.72532\n",
            "Epoch 8390: g1 has loss 0.50325 and g2 has loss 0.59171\n",
            "Epoch 8400: g1 has loss 0.53389 and g2 has loss 1.19175\n",
            "Epoch 8410: g1 has loss 2.36891 and g2 has loss 4.70707\n",
            "Epoch 8420: g1 has loss 4.22488 and g2 has loss 6.01500\n",
            "Epoch 8430: g1 has loss 0.85698 and g2 has loss 2.41082\n",
            "Epoch 8440: g1 has loss 1.67019 and g2 has loss 2.27070\n",
            "Epoch 8450: g1 has loss 0.92629 and g2 has loss 1.63082\n",
            "Epoch 8460: g1 has loss 0.36540 and g2 has loss 1.46459\n",
            "Epoch 8470: g1 has loss 0.46989 and g2 has loss 0.57356\n",
            "Epoch 8480: g1 has loss 1.38486 and g2 has loss 2.70623\n",
            "Epoch 8490: g1 has loss 1.30727 and g2 has loss 2.70287\n",
            "Epoch 8500: g1 has loss 4.38983 and g2 has loss 6.81895\n",
            "Epoch 8510: g1 has loss 0.16499 and g2 has loss 0.63309\n",
            "Epoch 8520: g1 has loss 1.77252 and g2 has loss 1.72721\n",
            "Epoch 8530: g1 has loss 2.39612 and g2 has loss 3.41680\n",
            "Epoch 8540: g1 has loss 6.27974 and g2 has loss 9.45755\n",
            "Epoch 8550: g1 has loss 3.68540 and g2 has loss 5.78285\n",
            "Epoch 8560: g1 has loss 0.55916 and g2 has loss 1.73013\n",
            "Epoch 8570: g1 has loss 7.08288 and g2 has loss 10.91997\n",
            "Epoch 8580: g1 has loss 4.03547 and g2 has loss 5.00362\n",
            "Epoch 8590: g1 has loss 1.20339 and g2 has loss 2.78664\n",
            "Epoch 8600: g1 has loss 0.95973 and g2 has loss 1.61158\n",
            "Epoch 8610: g1 has loss 6.15752 and g2 has loss 7.74484\n",
            "Epoch 8620: g1 has loss 2.24900 and g2 has loss 6.21373\n",
            "Epoch 8630: g1 has loss 0.35542 and g2 has loss 1.51214\n",
            "Epoch 8640: g1 has loss 2.08955 and g2 has loss 4.40616\n",
            "Epoch 8650: g1 has loss 4.02241 and g2 has loss 6.66586\n",
            "Epoch 8660: g1 has loss 2.70517 and g2 has loss 6.52475\n",
            "Epoch 8670: g1 has loss 0.91682 and g2 has loss 2.57002\n",
            "Epoch 8680: g1 has loss 10.30574 and g2 has loss 16.33583\n",
            "Epoch 8690: g1 has loss 3.27486 and g2 has loss 3.18315\n",
            "Epoch 8700: g1 has loss 0.83315 and g2 has loss 0.95694\n",
            "Epoch 8710: g1 has loss 2.41799 and g2 has loss 4.60193\n",
            "Epoch 8720: g1 has loss 0.42081 and g2 has loss 2.33512\n",
            "Epoch 8730: g1 has loss 4.90310 and g2 has loss 4.06971\n",
            "Epoch 8740: g1 has loss 7.91497 and g2 has loss 3.06765\n",
            "Epoch 8750: g1 has loss 5.25783 and g2 has loss 1.39811\n",
            "Epoch 8760: g1 has loss 2.19671 and g2 has loss 0.48453\n",
            "Epoch 8770: g1 has loss 3.01574 and g2 has loss 1.94797\n",
            "Epoch 8780: g1 has loss 1.62268 and g2 has loss 3.11126\n",
            "Epoch 8790: g1 has loss 3.70433 and g2 has loss 7.97534\n",
            "Epoch 8800: g1 has loss 1.10713 and g2 has loss 2.35369\n",
            "Epoch 8810: g1 has loss 7.19371 and g2 has loss 7.49859\n",
            "Epoch 8820: g1 has loss 4.01112 and g2 has loss 4.27309\n",
            "Epoch 8830: g1 has loss 2.32290 and g2 has loss 0.73002\n",
            "Epoch 8840: g1 has loss 0.81001 and g2 has loss 0.92478\n",
            "Epoch 8850: g1 has loss 1.21708 and g2 has loss 2.40272\n",
            "Epoch 8860: g1 has loss 4.22612 and g2 has loss 3.35886\n",
            "Epoch 8870: g1 has loss 1.59600 and g2 has loss 0.59158\n",
            "Epoch 8880: g1 has loss 0.65240 and g2 has loss 1.53013\n",
            "Epoch 8890: g1 has loss 5.88346 and g2 has loss 10.90471\n",
            "Epoch 8900: g1 has loss 0.95220 and g2 has loss 2.90898\n",
            "Epoch 8910: g1 has loss 0.29354 and g2 has loss 0.48923\n",
            "Epoch 8920: g1 has loss 1.70784 and g2 has loss 2.53108\n",
            "Epoch 8930: g1 has loss 0.41716 and g2 has loss 2.42592\n",
            "Epoch 8940: g1 has loss 0.46543 and g2 has loss 1.42081\n",
            "Epoch 8950: g1 has loss 1.16323 and g2 has loss 1.49845\n",
            "Epoch 8960: g1 has loss 0.73005 and g2 has loss 2.49552\n",
            "Epoch 8970: g1 has loss 3.32528 and g2 has loss 6.16100\n",
            "Epoch 8980: g1 has loss 1.27135 and g2 has loss 2.84157\n",
            "Epoch 8990: g1 has loss 1.60111 and g2 has loss 4.16975\n",
            "Epoch 9000: g1 has loss 0.98452 and g2 has loss 0.87416\n",
            "Epoch 9010: g1 has loss 0.07259 and g2 has loss 0.11408\n",
            "Epoch 9020: g1 has loss 1.36099 and g2 has loss 0.96006\n",
            "Epoch 9030: g1 has loss 0.19751 and g2 has loss 0.47733\n",
            "Epoch 9040: g1 has loss 4.47185 and g2 has loss 5.93030\n",
            "Epoch 9050: g1 has loss 5.00483 and g2 has loss 8.98244\n",
            "Epoch 9060: g1 has loss 1.14718 and g2 has loss 2.91582\n",
            "Epoch 9070: g1 has loss 2.55713 and g2 has loss 2.84894\n",
            "Epoch 9080: g1 has loss 6.07783 and g2 has loss 11.34667\n",
            "Epoch 9090: g1 has loss 0.24985 and g2 has loss 0.37247\n",
            "Epoch 9100: g1 has loss 0.00497 and g2 has loss 1.10198\n",
            "Epoch 9110: g1 has loss 1.01240 and g2 has loss 1.84673\n",
            "Epoch 9120: g1 has loss 1.56507 and g2 has loss 2.36682\n",
            "Epoch 9130: g1 has loss 5.28799 and g2 has loss 6.05712\n",
            "Epoch 9140: g1 has loss 0.97556 and g2 has loss 1.99938\n",
            "Epoch 9150: g1 has loss 1.81181 and g2 has loss 2.08000\n",
            "Epoch 9160: g1 has loss 1.45355 and g2 has loss 0.80948\n",
            "Epoch 9170: g1 has loss 2.11474 and g2 has loss 3.66911\n",
            "Epoch 9180: g1 has loss 0.19954 and g2 has loss 0.67055\n",
            "Epoch 9190: g1 has loss 3.28430 and g2 has loss 4.46445\n",
            "Epoch 9200: g1 has loss 0.12954 and g2 has loss 0.74718\n",
            "Epoch 9210: g1 has loss 0.02717 and g2 has loss 0.22435\n",
            "Epoch 9220: g1 has loss 1.52516 and g2 has loss 2.94016\n",
            "Epoch 9230: g1 has loss 4.75123 and g2 has loss 7.39657\n",
            "Epoch 9240: g1 has loss 1.18603 and g2 has loss 0.72150\n",
            "Epoch 9250: g1 has loss 0.81079 and g2 has loss 1.98471\n",
            "Epoch 9260: g1 has loss 2.81726 and g2 has loss 1.85647\n",
            "Epoch 9270: g1 has loss 5.04288 and g2 has loss 4.33837\n",
            "Epoch 9280: g1 has loss 2.31233 and g2 has loss 4.43489\n",
            "Epoch 9290: g1 has loss 0.37299 and g2 has loss 1.47295\n",
            "Epoch 9300: g1 has loss 0.23450 and g2 has loss 0.87832\n",
            "Epoch 9310: g1 has loss 0.61617 and g2 has loss 1.07841\n",
            "Epoch 9320: g1 has loss 4.22242 and g2 has loss 7.68974\n",
            "Epoch 9330: g1 has loss 1.42233 and g2 has loss 1.90241\n",
            "Epoch 9340: g1 has loss 0.25609 and g2 has loss 1.24295\n",
            "Epoch 9350: g1 has loss 0.37136 and g2 has loss 1.15989\n",
            "Epoch 9360: g1 has loss 0.20367 and g2 has loss 1.13556\n",
            "Epoch 9370: g1 has loss 1.29031 and g2 has loss 0.94787\n",
            "Epoch 9380: g1 has loss 0.94220 and g2 has loss 2.28204\n",
            "Epoch 9390: g1 has loss 4.55763 and g2 has loss 7.88070\n",
            "Epoch 9400: g1 has loss 0.40445 and g2 has loss 1.58349\n",
            "Epoch 9410: g1 has loss 0.30735 and g2 has loss 0.52071\n",
            "Epoch 9420: g1 has loss 0.45724 and g2 has loss 0.81854\n",
            "Epoch 9430: g1 has loss 2.74919 and g2 has loss 4.49592\n",
            "Epoch 9440: g1 has loss 3.74288 and g2 has loss 3.71223\n",
            "Epoch 9450: g1 has loss 4.06801 and g2 has loss 6.34113\n",
            "Epoch 9460: g1 has loss 0.83177 and g2 has loss 2.21601\n",
            "Epoch 9470: g1 has loss 5.34190 and g2 has loss 7.92011\n",
            "Epoch 9480: g1 has loss 6.42486 and g2 has loss 8.63431\n",
            "Epoch 9490: g1 has loss 2.49665 and g2 has loss 4.56555\n",
            "Epoch 9500: g1 has loss 0.17386 and g2 has loss 1.41726\n",
            "Epoch 9510: g1 has loss 2.11026 and g2 has loss 0.49535\n",
            "Epoch 9520: g1 has loss 3.91670 and g2 has loss 5.98080\n",
            "Epoch 9530: g1 has loss 2.80511 and g2 has loss 5.55124\n",
            "Epoch 9540: g1 has loss 1.47062 and g2 has loss 2.05076\n",
            "Epoch 9550: g1 has loss 2.81548 and g2 has loss 2.29443\n",
            "Epoch 9560: g1 has loss 1.28793 and g2 has loss 2.42320\n",
            "Epoch 9570: g1 has loss 14.27327 and g2 has loss 28.81652\n",
            "Epoch 9580: g1 has loss 3.49965 and g2 has loss 4.27733\n",
            "Epoch 9590: g1 has loss 3.38564 and g2 has loss 4.18663\n",
            "Epoch 9600: g1 has loss 21.99360 and g2 has loss 3.31672\n",
            "Epoch 9610: g1 has loss 36.05722 and g2 has loss 3.39077\n",
            "Epoch 9620: g1 has loss 0.82443 and g2 has loss 1.85699\n",
            "Epoch 9630: g1 has loss 1.22611 and g2 has loss 3.80148\n",
            "Epoch 9640: g1 has loss 4.73967 and g2 has loss 1.29292\n",
            "Epoch 9650: g1 has loss 0.15767 and g2 has loss 1.17687\n",
            "Epoch 9660: g1 has loss 7.87298 and g2 has loss 12.61566\n",
            "Epoch 9670: g1 has loss 0.70804 and g2 has loss 3.08925\n",
            "Epoch 9680: g1 has loss 3.50781 and g2 has loss 7.59885\n",
            "Epoch 9690: g1 has loss 0.64002 and g2 has loss 1.18898\n",
            "Epoch 9700: g1 has loss 4.67194 and g2 has loss 7.30052\n",
            "Epoch 9710: g1 has loss 2.83336 and g2 has loss 2.08563\n",
            "Epoch 9720: g1 has loss 16.53686 and g2 has loss 12.01853\n",
            "Epoch 9730: g1 has loss 7.52395 and g2 has loss 1.44689\n",
            "Epoch 9740: g1 has loss 3.87969 and g2 has loss 3.56009\n",
            "Epoch 9750: g1 has loss 2.24123 and g2 has loss 1.82622\n",
            "Epoch 9760: g1 has loss 0.56588 and g2 has loss 1.70145\n",
            "Epoch 9770: g1 has loss 2.11896 and g2 has loss 4.41768\n",
            "Epoch 9780: g1 has loss 3.13699 and g2 has loss 3.87077\n",
            "Epoch 9790: g1 has loss 0.82674 and g2 has loss 1.87800\n",
            "Epoch 9800: g1 has loss 0.40776 and g2 has loss 2.20559\n",
            "Epoch 9810: g1 has loss 6.55999 and g2 has loss 2.85974\n",
            "Epoch 9820: g1 has loss 0.98849 and g2 has loss 1.70437\n",
            "Epoch 9830: g1 has loss 0.16274 and g2 has loss 1.15380\n",
            "Epoch 9840: g1 has loss 5.03969 and g2 has loss 4.43425\n",
            "Epoch 9850: g1 has loss 1.71333 and g2 has loss 2.49752\n",
            "Epoch 9860: g1 has loss 0.25951 and g2 has loss 1.14745\n",
            "Epoch 9870: g1 has loss 1.13904 and g2 has loss 3.16547\n",
            "Epoch 9880: g1 has loss 0.47212 and g2 has loss 1.13991\n",
            "Epoch 9890: g1 has loss 5.05525 and g2 has loss 7.16794\n",
            "Epoch 9900: g1 has loss 1.20286 and g2 has loss 3.23455\n",
            "Epoch 9910: g1 has loss 3.58123 and g2 has loss 5.95959\n",
            "Epoch 9920: g1 has loss 0.47760 and g2 has loss 1.34854\n",
            "Epoch 9930: g1 has loss 0.59600 and g2 has loss 1.74153\n",
            "Epoch 9940: g1 has loss 2.15128 and g2 has loss 1.57867\n",
            "Epoch 9950: g1 has loss 5.80466 and g2 has loss 7.71117\n",
            "Epoch 9960: g1 has loss 0.87033 and g2 has loss 1.86330\n",
            "Epoch 9970: g1 has loss 0.28364 and g2 has loss 0.96978\n",
            "Epoch 9980: g1 has loss 0.15498 and g2 has loss 0.85257\n",
            "Epoch 9990: g1 has loss 3.84470 and g2 has loss 3.01678\n",
            "Epoch 10000: g1 has loss 0.42252 and g2 has loss 0.71487\n",
            "\n",
            "##########################################################################################\n",
            "\n",
            "After running for 10000 epochs, g1 has final loss 0.42252 and g2 has final loss 0.71487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0y8XFLUcS-o"
      },
      "source": [
        "M_valid = 2000\n",
        "h_valid = (x_h - x_l) / M_valid\n",
        "x_valid = np.arange(x_l, x_h, h_valid)\n",
        "\n",
        "g1_pred = g1(x_valid.reshape(-1,1))[:,0]\n",
        "g2_pred = g2(x_valid.reshape(-1,1))[:,0]\n",
        "\n",
        "g1_valid = tf.sin(x_valid)\n",
        "g2_valid = 1 + (x_valid ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFsbWRJod3Wi",
        "outputId": "436512af-96d5-493c-b681-cd57bc56e6c9"
      },
      "source": [
        "# 2D plot for g1\n",
        "plt.figure(figsize = (5, 5))\n",
        "plt.plot(x_valid, g1_pred, 'r', label='G1_predicted')\n",
        "plt.plot(x_valid, g1_valid, 'b', label='G1_solution')\n",
        "plt.plot(x_valid, g2_pred, 'r', label='G2_predicted')\n",
        "plt.plot(x_valid, g2_valid, 'b', label='G2_solution')\n",
        "plt.legend()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('G1')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}